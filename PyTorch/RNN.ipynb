{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 循环神经网络（Recurrent Neural Network，RNN）",
   "id": "f01522bec712a910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.概述\n",
    "RNN 是一种深度学习模型，它能够处理序列数据，如文本、音频、视频等。它可以记住之前的输入，并利用这些信息对当前输入做出更好的预测。RNNs 通常由多个隐藏层组成，每个隐藏层中都有多个神经元，每个神经元与前一时刻的输出和当前时刻的输入相连。RNNs 能够捕捉时间序列数据中的长期依赖关系，并对未来数据进行预测。"
   ],
   "id": "745cf19e08a11cb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 词嵌入层\n",
    "词嵌入层的作用就是将文本转换为向量。词嵌入层首先会根据输入的词的数量构建一个词向量矩阵，每一行代表一个词，每一列代表一个词向量（维数自定）。\n",
    "\n",
    "在 PyTorch 中，我们可以使用 nn.Embedding 词嵌入层来实现输入词的向量化。\n",
    "\n",
    "具体步骤：\n",
    "1. Tokenize 输入的文本，构建词与词索引的映射关系。\n",
    "2. 使用 nn.Embedding 构建词嵌入矩阵，词索引对应的向量即为该词对应的数值化后的向量表示。\n",
    "\n",
    "注意：词嵌入层中的向量表示是可学习的，并不是固定不变的。"
   ],
   "id": "9832bc5cbf87d2d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T05:38:18.241299Z",
     "start_time": "2024-07-09T05:38:15.548046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import jieba\n",
    "\n",
    "text = '北京冬奥的进度条已经过半，不少外国运动员在完成自己的比赛后踏上归途。'\n",
    "# 1. Tokenize\n",
    "words = jieba.lcut(text)\n",
    "print(words)\n",
    "# 2. 构建词表\n",
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "unique_words = list(set(words))  # 去重\n",
    "\n",
    "for i, word in enumerate(unique_words):\n",
    "    index_to_word[i] = word\n",
    "    word_to_index[word] = i\n",
    "\n",
    "# 3. 构建词嵌入层\n",
    "'''\n",
    "num_embeddings: 词表大小\n",
    "embedding_dim: 词向量维度\n",
    "'''\n",
    "embed = nn.Embedding(num_embeddings=len(index_to_word), embedding_dim=4)\n",
    "print('-' * 50)\n",
    "# 4. 文本转换为词向量\n",
    "for word in words:\n",
    "    index = word_to_index[word]\n",
    "    vector = embed(torch.tensor(index))\n",
    "    print('%3s -> %s' % (word, vector))"
   ],
   "id": "ed0a45dbaba86bf5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Ignorant\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.576 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '冬奥', '的', '进度条', '已经', '过半', '，', '不少', '外国', '运动员', '在', '完成', '自己', '的', '比赛', '后', '踏上', '归途', '。']\n",
      "--------------------------------------------------\n",
      " 北京 -> tensor([-0.2493, -2.0450,  1.3385, -0.1441], grad_fn=<EmbeddingBackward0>)\n",
      " 冬奥 -> tensor([-0.3383,  0.8249,  0.0257, -0.9948], grad_fn=<EmbeddingBackward0>)\n",
      "  的 -> tensor([-0.9397, -0.0259,  0.5528,  0.2483], grad_fn=<EmbeddingBackward0>)\n",
      "进度条 -> tensor([-0.5763, -0.5205,  1.0997,  0.6395], grad_fn=<EmbeddingBackward0>)\n",
      " 已经 -> tensor([ 1.5045,  2.1313, -0.2308,  1.0335], grad_fn=<EmbeddingBackward0>)\n",
      " 过半 -> tensor([-2.4331, -2.2740,  0.9175,  0.0867], grad_fn=<EmbeddingBackward0>)\n",
      "  ， -> tensor([ 0.9867, -1.3687, -0.3180, -1.6667], grad_fn=<EmbeddingBackward0>)\n",
      " 不少 -> tensor([-0.1120,  1.5962,  0.0292,  0.3414], grad_fn=<EmbeddingBackward0>)\n",
      " 外国 -> tensor([-0.5455, -0.0541, -0.2291, -0.2168], grad_fn=<EmbeddingBackward0>)\n",
      "运动员 -> tensor([ 1.1138e+00, -1.5857e+00,  1.1528e+00, -1.4141e-03],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "  在 -> tensor([ 0.6435, -0.1537, -1.0716,  0.4285], grad_fn=<EmbeddingBackward0>)\n",
      " 完成 -> tensor([-0.8991, -1.0239,  1.9591,  0.6416], grad_fn=<EmbeddingBackward0>)\n",
      " 自己 -> tensor([-1.5166, -0.8436, -1.3463, -2.1693], grad_fn=<EmbeddingBackward0>)\n",
      "  的 -> tensor([-0.9397, -0.0259,  0.5528,  0.2483], grad_fn=<EmbeddingBackward0>)\n",
      " 比赛 -> tensor([-1.7215, -1.2087,  0.1696,  0.3667], grad_fn=<EmbeddingBackward0>)\n",
      "  后 -> tensor([-0.5527, -0.3020, -0.9742, -1.3280], grad_fn=<EmbeddingBackward0>)\n",
      " 踏上 -> tensor([0.1449, 0.1891, 0.4683, 1.7769], grad_fn=<EmbeddingBackward0>)\n",
      " 归途 -> tensor([ 0.1223,  0.2702, -0.3461,  0.8594], grad_fn=<EmbeddingBackward0>)\n",
      "  。 -> tensor([-0.4685,  0.0592, -0.5650,  1.5563], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.循环网络层",
   "id": "96e5de60ec4eee2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 RNN 网络原理\n",
    "网络结构：\n",
    "\n",
    "<img src=\"images/RNN.png\">\n",
    "\n",
    "其中 h 表示隐藏状态，x 表示输入，y 表示输出。每次输入包含两个值：上一个时间步的隐藏状态和当前时间步的输入。输出当前时间步的隐藏状态，并作为下一个时间步的输入。\n",
    "\n",
    "实际上上图所描述的过程只用了一个神经元。\n",
    "\n",
    "RNN 网络可以有多个神经元：\n",
    "\n",
    "<img src=\"images/RNN_multi.png\">\n",
    "\n",
    "依次将 \"你爱我\" 三个字分别送入到每个神经元进行计算，假设词嵌入时，\"你爱我\" 的维度为 128，经过循环网络之后，\"你爱我\" 三个字的词向量维度就会变成 4。\n",
    "\n",
    "每个神经元内部的计算公式：$$ h_t = tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh}) $$\n",
    "\n",
    "其中 $W_{ih}$ 和 $W_{hh}$ 分别是输入和隐藏状态的权重矩阵，$b_{ih}$ 和 $b_{hh}$ 分别是输入和隐藏状态的偏置项。"
   ],
   "id": "b348c065ea6c04b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T06:02:26.614232Z",
     "start_time": "2024-07-09T06:02:26.591946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RNN 送入单个数据\n",
    "rnn = nn.RNN(input_size=128, hidden_size=256)\n",
    "\n",
    "inputs = torch.randn(1, 1, 128)     # 输入形状为 (seq_len, batch, input_size)\n",
    "hn = torch.randn(1, 1, 256)\n",
    "\n",
    "output, hn = rnn(inputs, hn)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ],
   "id": "e0b837753392adaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T06:03:49.435340Z",
     "start_time": "2024-07-09T06:03:49.419296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RNN 送入批量数据\n",
    "rnn = nn.RNN(input_size=128, hidden_size=256)\n",
    "\n",
    "inputs = torch.randn(1, 32, 128)\n",
    "hn = torch.randn(1, 32, 256)\n",
    "\n",
    "output, hn = rnn(inputs, hn)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ],
   "id": "f94bbd0391e39650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256])\n",
      "torch.Size([1, 32, 256])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "70699b234c5e085e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
